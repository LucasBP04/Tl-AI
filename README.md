Esta es un Ia que usa sumo-gui para recrear trafico de lineas 
  SE NECESITA TENER INSATALDO sumo y descargar los codigos siguientes del proyecto por ultimo
  Tiene varias warnings las cuales son las siguientes no afectan al codigo pero si son para mejorar la Ia
        1-faltan luces amarrillas en los intercambions estas no afectan para nada a la ia
        2-algunos cambios son repentinos lo cual hace que unos veiculos fenen de repente esto se mejora agregando mas tiempo al semaforo 

  por ultimo lo siguiente es una gran descrripcion de como funciona
    üìÅ Archivos principales
1. Mi_red.net.xml

Es la red de SUMO.

Define:

La intersecci√≥n J5 (el sem√°foro).

Los edges (E3, E4, E5, E6.0, etc.).

El tlLogic del sem√°foro (fases, estados rojo/amarillo/verde).

Piensa en este archivo como ‚Äúel mapa + sem√°foro‚Äù de tu ciudad miniatura.

2. IA_gym.rou.xml

Es el archivo de rutas y flujos de tr√°fico.

Contiene:

route ‚Üí caminos que pueden seguir los veh√≠culos (por qu√© calles pasan).

flow ‚Üí cu√°nto tr√°fico llega desde cada direcci√≥n (periodo = cada cu√°ntos segundos entra un coche).

Aqu√≠ decides de d√≥nde viene mucho tr√°fico y de d√≥nde viene poco (formas las colas).

3. IA_gym.sumocfg

Es el archivo de configuraci√≥n de SUMO.

Le dice a SUMO:

Qu√© red cargar: Mi_red.net.xml.

Qu√© rutas/tr√°fico usar: IA_gym.rou.xml.

Es el archivo que se pasa a SUMO con la opci√≥n -c.

Es b√°sicamente el ‚Äúescenario‚Äù que se lanza en cada episodio.

4. env_sumo_dqn_j5.py

Define la clase SumoEnvDQN_J5, que es el entorno de RL.

Se encarga de:

Arrancar SUMO v√≠a TraCI con IA_gym.sumocfg.

Leer el estado del cruce:

n√∫mero de coches por direcci√≥n (N, E, S, O),

fase actual del sem√°foro.

Aplicar la acci√≥n de la IA:

0 ‚Üí mantener fase.

1 ‚Üí pasar a la siguiente fase.

Avanzar la simulaci√≥n unos segundos (delta_time).

Calcular la recompensa (negativo del tiempo de espera total).

Indicar cu√°ndo termina el episodio (cuando ya no quedan veh√≠culos).

Es el puente entre SUMO y el agente DQN.

5. dqn_agent.py

Implementa el agente DQN usando PyTorch.

Contiene:

La red neuronal (DQN) que aproxima la Q-funci√≥n.

El buffer de memoria (replay buffer).

La l√≥gica de:

pol√≠tica Œµ-greedy (select_action),

guardar transiciones (store),

actualizar la red (train_step),

copiar pesos a la red target (update_target).

Es el ‚Äúcerebro‚Äù que aprende a escoger fases del sem√°foro.

6. train_dqn_sumo.py

Es el script principal de entrenamiento.

Hace:

Crea el entorno SumoEnvDQN_J5.

Crea el agente DQNAgent.

Corre varios episodios:

state = env.reset()

action = agent.select_action(state)

next_state, reward, done, _ = env.step(action)

agent.store(...) y agent.train_step()

Cada episodio imprime la recompensa total y el valor de Œµ.

Es el archivo que ejecutas para entrenar:

python train_dqn_sumo.py

C√≥mo ejecutar

Aseg√∫rate de que todos los archivos est√©n en la misma carpeta del proyecto.

Edita train_dqn_sumo.py para que sumo_cfg apunte a tu ruta completa de IA_gym.sumocfg.

Desde esa carpeta (o con la ruta absoluta ya puesta), ejecuta:

python train_dqn_sumo.py


Si quieres ver la simulaci√≥n gr√°ficamente, en env_sumo_dqn_j5.py usa:

env = SumoEnvDQN_J5(sumo_cfg_path=sumo_cfg, use_gui=True, delta_time=5)
